# Q

## 概要

Q学習のプログラムが格納されたディレクトリです。

強化学習ではQ値と呼ばれる、行動を決める時の指標となる数値をもっています。
Q学習では、そのQ値を更新する時の式は以下のようになります。
$$
Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t}) + \alpha(r_{s_{t+1}} + \gamma max_{a_{t+1}}Q(s_{t+1}, a_{t+1}) - Q(s_{t}, a_{t}))
$$
ここで、$s$は状態、$a$は行動、$r$は報酬、$\alpha$は学習率、$\gamma$は割引率と呼ばれています。

ガンマがかかっている部分が特徴的な部分の一つで、次時刻に得られると見込まれている最大のQ値を利用して学習しています。

- q.pyがソースコードです。  
  問題設定としては、11$\times$19の講師空間の迷路をエージェントに解いてもらうようになっています。
  図1の0がエージェントが自由に行き来できる空間、1がスタート地点、2がゴール地点、3が落とし穴になっています。
  落とし穴に落ちたらマイナスの報酬が与えられ、スタートに戻されるようになっています。
  さらに、毎ステップで-1の報酬を与えているので、最短ルートを学習するようになっています。
  
  実行し学習が終了すると標準出力に図2のような図、同ディレクトリに「Trajectory_of_Agent-Q.png」という学習が終わった後のエージェントの軌跡をプロットした図3のようなグラフがプロットされます。
